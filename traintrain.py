#!/usr/bin/python
import time
import glob
import random
from pyspark import SparkContext
import numpy as np
import sys
import string

start_time = time.clock()

sc = SparkContext(appName = "malware-project")

batch = int(sys.argv[1])
#samplesize = 500 #int(sys.argv[2])

bfiles = glob.glob("project/b/*.gc")
mfiles = glob.glob("project/m/*.gc")
trainfiles = bfiles + mfiles

testfiles = trainfiles
#testfiles = glob.glob("project/t/*.gc")
#random.shuffle(trainfiles)
#random.shuffle(testfiles)

trainsize = len(trainfiles)
testsize = len(testfiles)

subjkds = glob.glob("traintrain/subjkd*.txt")

def jaccard(l1, l2):
    # compute jaccard coefficient
    j = float(len(set(l1).intersection(l2)))/float(len(set(l1).union(l2)))
    return j


def partdist(a):
    x,y = a
    batchx = min(batch,testsize-x)
    batchy = min(batch,trainsize-y)
    lines = [[] for i in range(batchx)]
    for i in range(batchx):
            f = open(testfiles[x + i])
            lines[i] = f.readlines()
            f.close()
    tlines = [[] for i in range(batchy)]
    for i in range(batchy):
            f = open(trainfiles[y + i])
            tlines[i] = f.readlines()
            f.close()
    
    subjkd = np.zeros((batchx,batchy))
    for i in range(batchx):
        for j in range(batchy):
            subjkd[i,j] = jaccard(lines[i], tlines[j])
    np.savetxt("subjkd-%d-%d.txt"%(x,y),subjkd, fmt ="%.6f", delimiter=" ")
    return subjkd, x, y , batchx, batchy



print trainsize
print testsize

# run everything in parallel in Spark

processed = []
for fn in subjkds:
	sfn = string.strip(fn,'.txt')
	prefix, x, y = string.split(sfn,'-')
	x = string.atoi(x)
	y = string.atoi(y)
	processed.append((x,y))

print len(processed)

alltasks = []
for x in range(testsize/batch+1):
    for y in range(trainsize/batch+1):
        alltasks.append((x*batch,y*batch))
# randomly re-order all tasks
# random.shuffle(alltasks)

left = set(alltasks)- set(alltasks).intersection(processed)

# parallelize - not sure this is correct :)
allresults = sc.parallelize(left).map(partdist).collect()

# big matrix for all results
jkd = np.zeros((testsize, trainsize))

# merge results
for result in allresults:
    subjkd, x, y, batchx, batchy = result
    jkd[x:x+batchx, y:y+batchy] = subjkd

sc.stop()


np.savetxt("jkd-train.txt", jkd, fmt="%.6f", delimiter=" ")

print("%.3f seconds \n" % (time.clock() - start_time))



